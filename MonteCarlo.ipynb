{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"1\">Reinforcement Learning: Algoritmo Monte Carlo</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo Monte Carlo per il **reinforcement learning** apprende direttamente da episodi di esperienza senza alcuna conoscenza preliminare delle transizioni. Qui, il componente casuale è il ritorno o la ricompensa.\n",
    "\n",
    "Un avvertimento è che questo metodo può essere applicato solo a problemi episodici. Il motivo è che l'episodio deve terminare prima di poter calcolare eventuali ritorni. Qui, non eseguiamo un aggiornamento dopo ogni azione, ma piuttosto dopo ogni episodio. Utilizza l'idea più semplice: il valore è il ritorno medio di tutte le traiettorie di esempio per ciascuno stato.\n",
    "\n",
    "Analogamente alla programmazione dinamica, vi è una valutazione regola (individuazione della funzione valore per una determinata regola casuale) e una fase di miglioramento della regola (ricerca della regola ottimale).\n",
    "\n",
    "### Valutazione della regola (policy evaluation)\n",
    "L'obiettivo qui è quello di imparare la funzione di valore vpi(s) da episodi di esperienza con una regola pi. Il rendimento è la ricompensa totale scontata. Possiamo stimare qualsiasi valore atteso semplicemente sommando i campioni e dividendo per il numero totale di campioni. \n",
    "\n",
    "La domanda è: come otteniamo questi ritorni campione? Per questo, abbiamo bisogno di generare ed eseguire un sacco di episodi. Per ogni episodio che eseguiamo, avremo una sequenza di stati e ricompense. E da queste ricompense, possiamo calcolare il ritorno che è solo la somma di tutti i premi futuri. \n",
    "\n",
    "Ci sono due tipi di algoritmi per policy evaluation:\n",
    "* **First visit Monte Carlo**: si calcola il redimento medio solo la prima volta che si vede l'episodio.\n",
    "* **Every visit Monte Carlo**: Si calcola il rendimento medio per ogni volta che viene visitato in un episodio.\n",
    "\n",
    "### Miglioramento regola (policy improvement)\n",
    "Una volta che abbiamo la funzione valore per una regola casuale, l'importante compito che rimane rimane quello di trovare la regola ottimale usando Monte Carlo. \n",
    "Il miglioramento delle regole viene fatto rendendo la regola avida rispetto alla funzione del valore corrente. In questo caso, abbiamo una funzione valore-azione e quindi non è necessario alcun modello per costruire la politica avida. Una regola avida favorirà sempre una certa azione se la maggior parte delle azioni non viene esplorata correttamente. Ci sono due soluzioni per questo:\n",
    "* **Monte Carlo con exploring starts**: Tutte le coppie di azioni e stato hanno probabilità diversa da zero di essere la coppia di partenza. Questo garantirà che ogni episodio riprodotto porterà l'agente in nuovi stati e quindi, c'è più esplorazione dell'ambiente.\n",
    "* **Monte Carlo con epsilon-Soft**: Cosa succede se esiste un unico punto di partenza per un ambiente (ad esempio, una partita a scacchi)? L'esplorazione degli inizi non è l'opzione giusta in questi casi. L'idea più semplice per garantire l'esplorazione continua. Tutte le azioni sono provate con probabilità diversa da zero (1 - epsilon) e viene scelta l'azione che massimizza la funzione del valore d'azione e poi con probabilità epsilon si sceglie un'azione a caso.\n",
    "\n",
    "Per lo sviluppo dell'algoritmo Monte Carlo ho tradotto lo pseudo codice, che si può trovare in qualsiasi sito internet, in python e applico questo algoritmo ad il caso discusso precedentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tecnica Montecarlo\n",
    "class Montecarlo:\n",
    "    \n",
    "    def __init__(self, environment, grid_delta):\n",
    "        '''\n",
    "        arguments:\n",
    "        grid_delta - passo per la discretizzazione delle osservazioni\n",
    "        '''\n",
    "        self._env = gym.make(environment)\n",
    "        self._grid_delta = grid_delta\n",
    "        self.q_dict = {}\n",
    "        \n",
    "    def run_episode(self, epsilon, sleep=None):\n",
    "        trajectory = []\n",
    "        o = self._env.reset()\n",
    "        s = self._get_state(o)\n",
    "        trajectory.append(s)\n",
    "        done = False\n",
    "        n = 0\n",
    "        while not done:\n",
    "            a = self._choose_action(s , epsilon)\n",
    "            o, r, done, _ = self._env.step(a)\n",
    "            #Per vedere quando impara: Attenzione il processo diventa molto più lento\n",
    "            #self._env.render()\n",
    "            s = self._get_state(o)\n",
    "            trajectory += [a,r,s]\n",
    "            n += 1\n",
    "        self.update_q(trajectory)\n",
    "        return n\n",
    "    \n",
    "    def learn(self, num_episodes, epsilon0, decay,  sleep=None):\n",
    "        counters = [] # lista con le durate degli episodi\n",
    "        epsilon = epsilon0\n",
    "        for i in range(num_episodes):\n",
    "            counters.append(self.run_episode(epsilon))\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\"epsilon={epsilon:.2f}, lunghezza media={sum(counters[-100:])/100}\")\n",
    "                epsilon = epsilon * decay\n",
    "        self._env.close()\n",
    "        \n",
    "    def _perform(self, sleep=None):\n",
    "        observation = self._env.reset()\n",
    "        state = self._get_state(observation)\n",
    "        self._env.render()\n",
    "        done = False\n",
    "        count = 0\n",
    "            \n",
    "#        while not done:\n",
    "        for _ in range(200):\n",
    "            if not done:\n",
    "                count = count + 1\n",
    "            action = self._choose_action(state, 0)\n",
    "            observation, reward, done, info = self._env.step(action)\n",
    "            state = self._get_state(observation)\n",
    "            self._env.render()\n",
    "            if sleep:\n",
    "                time.sleep(sleep)\n",
    "                \n",
    "        self._env.close()\n",
    "        print(count)\n",
    "        \n",
    "    def perform(self, n, sleep=None):\n",
    "        for _ in range(n):\n",
    "            self._perform(sleep)\n",
    "            \n",
    "    def close(self):\n",
    "        self._env.close()\n",
    "        \n",
    "    def reset_env(self):\n",
    "        self._env.reset()\n",
    "        \n",
    "    def _get_state(self, observation):\n",
    "        return tuple([value // self._grid_delta for value in observation.tolist()])\n",
    "    \n",
    "    def _choose_action(self, s, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return self._env.action_space.sample()\n",
    "        else:\n",
    "            return self.get_best_action(s)\n",
    "        \n",
    "    def get_best_action(self, s):\n",
    "        action_values = self.q_dict.get(s, None)\n",
    "        if action_values is None:\n",
    "            return self._env.action_space.sample()\n",
    "        i = np.argmax([tuple_[1] for tuple_ in action_values])\n",
    "        return action_values[i][0]\n",
    "    \n",
    "    def update_q(self, trajectory):\n",
    "        while len(trajectory) > 3:\n",
    "            s, a = trajectory[:2]\n",
    "            rewards = sum(trajectory[2::3])\n",
    "            self.update_q_s_a(s, a, rewards)\n",
    "            trajectory = trajectory[3:]\n",
    "            \n",
    "    def update_q_s_a(self, s, a, rewards):\n",
    "        found = False\n",
    "        s_list = self.q_dict.get(s, [])\n",
    "        for index, terna in enumerate(s_list):\n",
    "            if terna[0] == a:\n",
    "                found = True\n",
    "                s_list[index] = (a,  ((terna[1]*terna[2] + rewards) / (terna[2]+1)), terna[2]+1)\n",
    "        if not found:\n",
    "            s_list.append([a, rewards, 1])\n",
    "        self.q_dict[s] = s_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta che abbiamo l'algoritmo Monte Carlo pronto possiamo utilizzarlo per diversi ambienti. Come primo esempio vi mostro l'ambiente gym **CartPole-v0** che è quello che simula il video youtube visto in precedenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = Montecarlo(environment='CartPole-v0', grid_delta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.perform(n=5, sleep=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.learn(num_episodes=2_000, epsilon0=0.6, decay=0.7,  sleep = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.perform(n=10, sleep=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come secondo ambiente vi mostro il **Pendulum-v0**. Il problema dell'oscillazione del pendolo invertito è un problema classico nella letteratura di controllo. In questa versione del problema, il pendolo inizia in una posizione casuale e l'obiettivo è di farlo ruotare in modo che rimanga dritto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuliomori/gymenv/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "mc1 = Montecarlo(environment='Pendulum-v0', grid_delta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "mc1.perform(n=5, sleep=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon=0.60, lunghezza media=200.0\n",
      "epsilon=0.42, lunghezza media=200.0\n",
      "epsilon=0.29, lunghezza media=200.0\n",
      "epsilon=0.21, lunghezza media=200.0\n",
      "epsilon=0.14, lunghezza media=200.0\n",
      "epsilon=0.10, lunghezza media=200.0\n",
      "epsilon=0.07, lunghezza media=200.0\n",
      "epsilon=0.05, lunghezza media=200.0\n",
      "epsilon=0.03, lunghezza media=200.0\n",
      "epsilon=0.02, lunghezza media=200.0\n",
      "epsilon=0.02, lunghezza media=200.0\n",
      "epsilon=0.01, lunghezza media=200.0\n",
      "epsilon=0.01, lunghezza media=200.0\n",
      "epsilon=0.01, lunghezza media=200.0\n",
      "epsilon=0.00, lunghezza media=200.0\n",
      "epsilon=0.00, lunghezza media=200.0\n",
      "epsilon=0.00, lunghezza media=200.0\n",
      "epsilon=0.00, lunghezza media=200.0\n",
      "epsilon=0.00, lunghezza media=200.0\n",
      "epsilon=0.00, lunghezza media=200.0\n"
     ]
    }
   ],
   "source": [
    "mc1.learn(num_episodes=2_000, epsilon0=0.6, decay=0.7,  sleep = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "mc1.perform(n=10, sleep=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma non per tutti gli ambienti il metodo montecarlo è efficace. Possiamo prendere l'esempio di **Acrobot-v1**, Il sistema di acrobot comprende due giunti e due collegamenti, in cui viene attivata la giunzione tra i due collegamenti. Inizialmente, i collegamenti sono sospesi verso il basso e l'obiettivo è di far oscillare la fine del collegamento inferiore fino a una determinata altezza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mc2 = Montecarlo(environment='Acrobot-v1', grid_delta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "mc2.perform(n=5, sleep=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc2.learn(num_episodes=2_000, epsilon0=0.6, decay=0.7,  sleep = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc2.perform(n=10, sleep=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
